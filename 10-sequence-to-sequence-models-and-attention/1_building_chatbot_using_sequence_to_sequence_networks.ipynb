{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-building-chatbot-using-sequence-to-sequence-networks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO8HNumcHmSNgwXIWud77/s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-in-action/blob/10-sequence-to-sequence-models-and-attention/1_building_chatbot_using_sequence_to_sequence_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJyz9isfCjnv",
        "colab_type": "text"
      },
      "source": [
        "# Building a chatbot using sequence-to-sequence networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGaQtavyCzFt",
        "colab_type": "text"
      },
      "source": [
        "We guide you through how to apply the various steps to train a chatbot. For the chatbot training, you’ll use the [Cornell movie dialog corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). You’ll train a sequenceto- sequence network to “adequately” reply to your questions or statements. Our chatbot example is an adopted sequence-to-sequence example from the [Keras blog](https://github.com/fchollet/keras/blob/master/examples/lstm_seq2seq.py).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5-PMsuYDLAD",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD2k3lc3DMXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import backend as keras_backend\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, LSTM\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "import re\n",
        "import tqdm\n",
        "\n",
        "import glob\n",
        "from random import shuffle\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "import requests"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiwN-ZGkFFSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = keras.utils.get_file('cornell_movie_dialogs_corpus.zip', \n",
        "                            origin='http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\n",
        "                            extract=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fPW8o7UD6L2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "0788adbb-0854-4769-9ec6-66c14ab30e2f"
      },
      "source": [
        "data_path"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.keras/datasets/cornell_movie_dialogs_corpus.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT9i2mt4FUpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /root/.keras/datasets/cornell_movie_dialogs_corpus.zip ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBCzmPYIIp_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('cornell_movie_dialogs_corpus.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('cornell_movie_dialogs_corpus')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu5OFH6YDNME",
        "colab_type": "text"
      },
      "source": [
        "## Preparing the corpus for your training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSvyn0o_DRGf",
        "colab_type": "text"
      },
      "source": [
        "First, you need to load the corpus and generate the training sets from it. The training data will determine the set of characters the encoder and decoder will support during the training and during the generation phase. Please note that this implementation doesn’t support characters that haven’t been included during the training phase.\n",
        "\n",
        "Using the entire Cornell Movie Dialog dataset can be computationally intensive because a few sequences have more than 2,000 tokens—2,000 time steps will take a while to unroll. But the majority of dialog samples are based on less than 100 characters.\n",
        "\n",
        "For this example, you’ve preprocessed the dialog corpus by limiting samples to those with fewer than 100 characters, removed odd characters, and only allowed lowercase characters.\n",
        "\n",
        "You’ll loop over the corpus file and generate the training pairs (technically 3-tuples: input text, target text with start token, and target text). While reading the corpus, you’ll also generate a set of input and target characters, which you’ll then use to onehot encode the samples. The input and target characters don’t have to match. \n",
        "\n",
        "But characters that aren’t included in the sets can’t be read or generated during the generation phase. The result of the following listing is two lists of input and target texts (strings), as well as two sets of characters that have been seen in the training corpus."
      ]
    }
  ]
}