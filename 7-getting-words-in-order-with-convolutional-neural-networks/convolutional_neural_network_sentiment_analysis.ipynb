{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convolutional-neural-network-sentiment-analysis.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-in-action/blob/7-text-with-convolutional-neural-networks/convolutional_neural_network_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO1preWdp2QJ",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Neural Network for Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfJtCbhbqTT1",
        "colab_type": "text"
      },
      "source": [
        "Let’s take a look at convolution in Python with the example convolutional neural network classifier provided in the Keras documentation. They have crafted a onedimensional convolutional net to examine the IMDB movie review dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8dMuJFaqjr2",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shf2mXZKrZg4",
        "colab_type": "code",
        "outputId": "a2ecd63e-b257-4eaa-8f38-dc3ac3ecf996",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import backend as keras_backend\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Conv1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "import re\n",
        "import tqdm\n",
        "\n",
        "import requests"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woc_3WGcp50m",
        "colab_type": "code",
        "outputId": "cf855d6b-3645-4dc0-9075-f157b40e936f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! pip install pugnlp"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pugnlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c6/17a0ef5af34e20b595f1983be0468efa9f903a17a5af2a0a980ecaf9c411/pugnlp-0.2.5-py2.py3-none-any.whl (706kB)\n",
            "\u001b[K     |████████████████████████████████| 716kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from pugnlp) (3.2.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from pugnlp) (3.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pugnlp) (4.28.1)\n",
            "Requirement already satisfied: wheel in /tensorflow-2.1.0/python3.6 (from pugnlp) (0.33.6)\n",
            "Collecting python-Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pugnlp) (3.1.2)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from pugnlp) (1.0.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from pugnlp) (4.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pugnlp) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pugnlp) (1.3.3)\n",
            "Collecting pypandoc\n",
            "  Downloading https://files.pythonhosted.org/packages/71/81/00184643e5a10a456b4118fc12c96780823adb8ed974eb2289f29703b29b/pypandoc-1.4.tar.gz\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from pugnlp) (4.1.1)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (from pugnlp) (19.3.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from pugnlp) (0.9.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pugnlp) (0.21.3)\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading https://files.pythonhosted.org/packages/d8/f1/5a267addb30ab7eaa1beab2b9323073815da4551076554ecc890a3595ec9/fuzzywuzzy-0.17.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: coverage in /usr/local/lib/python3.6/dist-packages (from pugnlp) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pugnlp) (0.25.3)\n",
            "Requirement already satisfied: six in /tensorflow-2.1.0/python3.6 (from nltk->pugnlp) (1.13.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->pugnlp) (1.9.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /tensorflow-2.1.0/python3.6 (from gensim->pugnlp) (1.17.4)\n",
            "Requirement already satisfied: setuptools in /tensorflow-2.1.0/python3.6 (from python-Levenshtein->pugnlp) (42.0.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pugnlp) (2.4.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pugnlp) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pugnlp) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pugnlp) (2.6.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->pugnlp) (7.5.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->pugnlp) (5.2.2)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->pugnlp) (5.2.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->pugnlp) (5.6.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->pugnlp) (4.6.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->pugnlp) (4.6.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->pugnlp) (1.3)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->pugnlp) (1.3.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pugnlp) (0.14.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->pugnlp) (2018.9)\n",
            "Requirement already satisfied: requests in /tensorflow-2.1.0/python3.6 (from smart-open>=1.2.1->gensim->pugnlp) (2.22.0)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->pugnlp) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->pugnlp) (1.10.40)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->pugnlp) (4.4.0)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->pugnlp) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->pugnlp) (4.3.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->pugnlp) (3.5.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->pugnlp) (0.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->pugnlp) (2.10.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->pugnlp) (5.3.4)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->pugnlp) (4.6.1)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->pugnlp) (0.8.3)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->pugnlp) (4.5.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->pugnlp) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->pugnlp) (2.1.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->pugnlp) (0.4.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->pugnlp) (0.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->pugnlp) (1.4.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->pugnlp) (3.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->pugnlp) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->pugnlp) (0.8.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /tensorflow-2.1.0/python3.6 (from requests->smart-open>=1.2.1->gensim->pugnlp) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /tensorflow-2.1.0/python3.6 (from requests->smart-open>=1.2.1->gensim->pugnlp) (2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /tensorflow-2.1.0/python3.6 (from requests->smart-open>=1.2.1->gensim->pugnlp) (1.25.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /tensorflow-2.1.0/python3.6 (from requests->smart-open>=1.2.1->gensim->pugnlp) (2019.11.28)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->pugnlp) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->pugnlp) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->pugnlp) (1.13.40)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->pugnlp) (2.6.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->pugnlp) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->pugnlp) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->pugnlp) (4.4.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->pugnlp) (4.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->jupyter->pugnlp) (1.1.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->jupyter->pugnlp) (17.0.0)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->pugnlp) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->pugnlp) (0.1.7)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->pugnlp) (0.5.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->smart-open>=1.2.1->gensim->pugnlp) (0.15.2)\n",
            "Building wheels for collected packages: python-Levenshtein, pypandoc\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144664 sha256=35d99e7791463735bbf628fc92557fe7d059e3aaf99f59fba19226ea80289526\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "  Building wheel for pypandoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypandoc: filename=pypandoc-1.4-cp36-none-any.whl size=16716 sha256=dca553daa499d59ccaf2b0aed0c12bacd4b836eb536df5ac8d439b6e62fa189e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/55/4f/59e0fa0914f3db52e87c0642c5fb986871dfbbf253026e639f\n",
            "Successfully built python-Levenshtein pypandoc\n",
            "Installing collected packages: python-Levenshtein, pypandoc, fuzzywuzzy, pugnlp\n",
            "Successfully installed fuzzywuzzy-0.17.0 pugnlp-0.2.5 pypandoc-1.4 python-Levenshtein-0.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evwKZIKdp6h3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pugnlp.futil import path_status, find_files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swxx7Tmxr0np",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQymxGOrr3X5",
        "colab_type": "text"
      },
      "source": [
        "Each data point is prelabeled with a 0 (negative sentiment) or a 1 (positive sentiment).you’re going to swap out their example IMDB movie review dataset\n",
        "for one in raw text, so you can get your hands dirty with the preprocessing of the text as well. And then you’ll see if you can use this trained network to classify text it has never seen before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8RMv-lktomF",
        "colab_type": "text"
      },
      "source": [
        "### Downloading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1jLX7hOrsr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BIG_URLS = {\n",
        "    'w2v': ('https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1', 1647046227),\n",
        "    'slang': ('https://www.dropbox.com/s/43c22018fbfzypd/slang.csv.gz?dl=1', 117633024),\n",
        "    'tweets': ('https://www.dropbox.com/s/5gpb43c494mc8p0/tweets.csv.gz?dl=1', 311725313),\n",
        "    'lsa_tweets': ('https://www.dropbox.com/s/rpjt0d060t4n1mr/lsa_tweets_5589798_2003588x200.tar.gz?dl=1', 3112841563),  # 3112841312\n",
        "    'imdb': ('https://www.dropbox.com/s/yviic64qv84x73j/aclImdb_v1.tar.gz?dl=1', 3112841563),  # 3112841312\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHaBlbdwtXG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These functions are part of the nlpia package which can be pip installed and run from there.\n",
        "def dropbox_basename(url):\n",
        "    filename = os.path.basename(url)\n",
        "    match = re.findall(r'\\?dl=[0-9]$', filename)\n",
        "    if match:\n",
        "        return filename[:-len(match[0])]\n",
        "    return filename\n",
        "\n",
        "def download_file(url, data_path='.', filename=None, size=None, chunk_size=4096, verbose=True):\n",
        "    \"\"\"Uses stream=True and a reasonable chunk size to be able to download large (GB) files over https\"\"\"\n",
        "    if filename is None:\n",
        "        filename = dropbox_basename(url)\n",
        "    file_path = os.path.join(data_path, filename)\n",
        "    if url.endswith('?dl=0'):\n",
        "        url = url[:-1] + '1'  # noninteractive download\n",
        "    if verbose:\n",
        "        tqdm_prog = tqdm\n",
        "        print('requesting URL: {}'.format(url))\n",
        "    else:\n",
        "        tqdm_prog = no_tqdm\n",
        "    r = requests.get(url, stream=True, allow_redirects=True)\n",
        "    size = r.headers.get('Content-Length', None) if size is None else size\n",
        "    print('remote size: {}'.format(size))\n",
        "\n",
        "    stat = path_status(file_path)\n",
        "    print('local size: {}'.format(stat.get('size', None)))\n",
        "    if stat['type'] == 'file' and stat['size'] == size:  # TODO: check md5 or get the right size of remote file\n",
        "        r.close()\n",
        "        return file_path\n",
        "\n",
        "    print('Downloading to {}'.format(file_path))\n",
        "\n",
        "    with open(file_path, 'wb') as f:\n",
        "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "            if chunk:  # filter out keep-alive chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "    r.close()\n",
        "    return file_path\n",
        "\n",
        "def untar(fname):\n",
        "    if fname.endswith(\"tar.gz\"):\n",
        "        with tarfile.open(fname) as tf:\n",
        "            tf.extractall()\n",
        "    else:\n",
        "        print(\"Not a tar.gz file: {}\".format(fname))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0oWrTZ1uFHh",
        "colab_type": "code",
        "outputId": "78773926-ea0c-40ab-875a-aeef1a8c003b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "download_file(BIG_URLS['w2v'][0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "requesting URL: https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1\n",
            "remote size: 1647046227\n",
            "local size: None\n",
            "Downloading to ./GoogleNews-vectors-negative300.bin.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./GoogleNews-vectors-negative300.bin.gz'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I09TKCmzuF6_",
        "colab_type": "code",
        "outputId": "07f90269-c643-4045-9ecb-ad92a1b052d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "untar(download_file(BIG_URLS['imdb'][0]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "requesting URL: https://www.dropbox.com/s/yviic64qv84x73j/aclImdb_v1.tar.gz?dl=1\n",
            "remote size: 84125825\n",
            "local size: None\n",
            "Downloading to ./aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipv8IOS3v6AK",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing the loaded documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjVYBXqrwT7V",
        "colab_type": "text"
      },
      "source": [
        "The reviews in the train folder are broken up into text files in either the pos or neg folders. You’ll first need to read those in Python with their appropriate label and then shuffle the deck so the samples aren’t all positive and then all negative. Training with the sorted labels will skew training toward whatever comes last, especially when you use certain hyperparameters, such as momentum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtmvAhqfv5Mo",
        "colab_type": "code",
        "outputId": "8f871e6b-59ae-41de-ef59-968a76b4aa63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "import glob\n",
        "from random import shuffle\n",
        "\n",
        "def pre_process_data(filepath):\n",
        "  '''\n",
        "  This is dependent on your training data source but we will try to generalize it as best as possible.\n",
        "  '''\n",
        "  positive_path = os.path.join(filepath, 'pos')\n",
        "  negative_path = os.path.join(filepath, 'neg')\n",
        "\n",
        "  pos_label = 1\n",
        "  neg_label = 0\n",
        "\n",
        "  dataset = []\n",
        "\n",
        "  for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
        "    with open(filename, 'r') as f:\n",
        "      dataset.append((pos_label, f.read()))\n",
        "\n",
        "  for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
        "    with open(filename, 'r') as f:\n",
        "      dataset.append((neg_label, f.read()))\n",
        "\n",
        "  shuffle(dataset)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "dataset = pre_process_data('./aclImdb/train')\n",
        "print(dataset[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, \"I saw this film at the Toronto International Film Festival. Not as salacious as it sounds, this is a three-part documentary (each episode is 50 minutes) featuring Slovenian superstar philosopher/psychoanalyst Slavoj Zizek. Zizek takes us on a journey through many classic films, exploring themes of sexuality, fantasy, morality and mortality. It was directed by Sophie Fiennes, of the multi-talented Fiennes clan (she's sister to actors Ralph and Joseph).<br /><br />I enjoyed this quite a bit, although I think it will be even more enjoyable on DVD, since there is such a stew of ideas to be digested. Freudian and Lacanian analysis can be pretty heavy going and seeing the whole series all at once became a bit disorienting by the end of two and a half hours. It didn't help that an ill-advised coffee and possession of a bladder led me to some discomfort for the last hour or so.<br /><br />My only real issue with this is that Zizek picked films that were quite obviously filled with Freudian themes. He spends quite a bit of time on the films of Hitchcock and David Lynch, not exactly masters of subtlety. I would have liked to see him try to support his theories by using a wider range of films, although that's really just me saying I'd like to see part four and five and six.<br /><br />Zizek is very funny, and part of the humour was watching him present what amounted to a lecture while inserting himself into the actual scenes from some of the films he's discussing. So, for instance, we see him in a motorboat on his way to Bodega Bay (from Hitchcock's The Birds) or sitting in the basement of the Bates Motel (from Psycho). Which is not to say that his theories are not provocative. Even when I found myself disagreeing with him, it definitely made me think a little more deeply about the films. Which is exactly what he's trying to accomplish.\")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W76Jm0mbGm3o",
        "colab_type": "text"
      },
      "source": [
        "### Data tokenization and vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT3S4Gd3FCvO",
        "colab_type": "text"
      },
      "source": [
        "The next step is to tokenize and vectorize the data. You’ll use the Google News pretrained Word2vec vectors, so download those directly from Google.\n",
        "\n",
        "You’ll use gensim to unpack the vectors, You can\n",
        "experiment with the limit argument to the load_word2vec_format method; a\n",
        "higher number will get you more vectors to play with, but memory quickly becomes an issue and return on investment drops quickly in really high values for limit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slErmWLvxsca",
        "colab_type": "code",
        "outputId": "d9acf3e7-a0ec-4490-f867-d108ea26f40b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True, limit=200000)\n",
        "\n",
        "def tokenize_and_vectorize(dataset):\n",
        "  tokenizer = TreebankWordTokenizer()\n",
        "  vectorized_data = []\n",
        "  expected = []\n",
        "\n",
        "  for sample in dataset:\n",
        "    tokens = tokenizer.tokenize(sample[1])\n",
        "    sample_vecs = []\n",
        "    for token in tokens:\n",
        "      try:\n",
        "        sample_vecs.append(word_vectors[token])\n",
        "      except KeyError:\n",
        "        pass    # No matching token in the Google w2v vocab\n",
        "\n",
        "    vectorized_data.append(sample_vecs)\n",
        "\n",
        "  return vectorized_data"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1Z3bpwfIfyq",
        "colab_type": "text"
      },
      "source": [
        "You also need to collect the target values—0 for a negative review, 1 for a positive review—in the same order as the training samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC1qJoaNH5gF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_expected(dataset):\n",
        "  '''Peel of the target values from the dataset'''\n",
        "  expected = []\n",
        "  for sample in dataset:\n",
        "    expected.append(sample[0])\n",
        "  \n",
        "  return expected"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAwhZvflImSK",
        "colab_type": "text"
      },
      "source": [
        "And then you simply pass your data into those functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu5lfSiGIMwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorized_data = tokenize_and_vectorize(dataset)\n",
        "expected = collect_expected(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u65UIysaI_cP",
        "colab_type": "text"
      },
      "source": [
        "### Train/Test splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EgeyjZgJHu4",
        "colab_type": "text"
      },
      "source": [
        "Next you’ll split the prepared data into a training set and a test set. You’re just going to split your imported dataset 80/20, but this ignores the folder of test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuO_XoY4JQd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_point = int(len(vectorized_data) * .8)\n",
        "\n",
        "x_train = vectorized_data[:split_point]\n",
        "y_train = expected[:split_point]\n",
        "x_test = vectorized_data[split_point:]\n",
        "y_test = expected[split_point:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERwTnAjrKccV",
        "colab_type": "text"
      },
      "source": [
        "### CNN parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ_9-wsmKReT",
        "colab_type": "text"
      },
      "source": [
        "The next sets most of the hyperparameters for the net."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2Y0InHSKqGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen = 400          # holds the maximum review length\n",
        "batch_size = 32       # How many samples to show the net before backpropagating the error and updating the weights\n",
        "embedding_dims = 300  # Length of the token vectors you’ll create for passing into the convnet\n",
        "filters = 250         # Number of filters you’ll train\n",
        "kernel_size = 3       # Filters width; actual filters will each be a matrix of weights of size: embedding_dims x kernel_size, or 50 x 3 in your case\n",
        "hidden_dims = 250     # Number of neurons in the plain feed forward net at the end of the chain\n",
        "epochs = 2            # Number of times we will pass the entire training dataset through the network"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qAveehRMnBK",
        "colab_type": "text"
      },
      "source": [
        "### Padding and truncating token sequence(sequences of vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di-Sa4YOMshT",
        "colab_type": "text"
      },
      "source": [
        "Keras has a preprocessing helper method, pad_sequences, that in theory could be\n",
        "used to pad your input data, but unfortunately it works only with sequences of scalars, and you have sequences of vectors. \n",
        "\n",
        "Let’s write a helper function of your own to pad your input data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v51zAgevM0Kq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_trunc(data, maxlen):\n",
        "  '''For a given dataset pad with zero vectors or truncate to maxlen'''\n",
        "  new_data = []\n",
        "\n",
        "  # Create a vector of 0's the length of our word vectors\n",
        "  zero_vector = []\n",
        "  for _ in range(len(data[0][0])):\n",
        "    zero_vector.append(0.0)\n",
        "  #zero_vector = [0.0 for _ in range(len(data[0][0]))]\n",
        "\n",
        "  for sample in data:\n",
        "    if len(sample) > maxlen:\n",
        "        temp = sample[:maxlen]\n",
        "    elif len(sample) < maxlen:\n",
        "        temp = sample\n",
        "        additional_elems = maxlen - len(sample)\n",
        "        for _ in range(additional_elems):\n",
        "            temp.append(zero_vector)\n",
        "    else:\n",
        "        temp = sample\n",
        "    new_data.append(temp)\n",
        "  \n",
        "  return new_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKWpu1TYODk2",
        "colab_type": "text"
      },
      "source": [
        "Then you need to pass your train and test data into the padder/truncator. After that you can convert it to numpy arrays to make Keras happy. This is a tensor with the shape (number of samples, sequence length, word vector length) that you need for your CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGkviQPlFb_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = pad_trunc(x_train, maxlen)\n",
        "keras_backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNQZ3p6g7Wng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = pad_trunc(x_test, maxlen)\n",
        "keras_backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6qc3GwKNxGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
        "y_train = np.array(y_train)\n",
        "keras_backend.clear_session()\n",
        "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mj96pjMOvxX",
        "colab_type": "text"
      },
      "source": [
        "Phew; finally you’re ready to build a neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8CB1D4XOwQm",
        "colab_type": "text"
      },
      "source": [
        "## Convolutional neural network architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ildkqgWOyny",
        "colab_type": "text"
      },
      "source": [
        "Sequential is one of the base classes for neural networks in Keras. From here you can start to layer on the magic.\n",
        "\n",
        "The first piece you add is a convolutional layer. In this case, you assume that it’s okay that the output is of smaller dimension than the input, and you set the padding to 'valid'. Each filter will start its pass with its leftmost edge at the start of the sentence and stop with its rightmost edge on the last token.\n",
        "\n",
        "Each shift (stride) in the convolution will be one token. The kernel (window\n",
        "width) you already set to three tokens.And you’re using the 'relu' activation\n",
        "function. At each step, you’ll multiply the filter weight times the value in the\n",
        "three tokens it’s looking at (element-wise), sum up those answers, and pass them through if they’re greater than 0, else you output 0. That last passthrough of positive values and 0s is the rectified linear units activation function or ReLU.\n",
        "\n",
        "```python\n",
        "model = Sequential()\n",
        "# Add one Conv1D layer, which will learn word group filters of size kernel_size.\n",
        "model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1, input_shape=(maxlen, embedding_dims)))\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JRIDUL3uZbW",
        "colab_type": "text"
      },
      "source": [
        "### Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aCxJgxwuahb",
        "colab_type": "text"
      },
      "source": [
        "Pooling is the convolutional neural network’s path to dimensionality reduction. In some ways, you’re speeding up the process by allowing for parallelization of the computation.\n",
        "\n",
        "The key idea is you’re going to evenly divide the output of each filter into a subsection. Then for each of those subsections, you’ll select or compute a representative value. And then you set the original output aside and use the collections of representative values as the input to the next layers.\n",
        "\n",
        "Usually, discarding data wouldn’t be the best course of action. But it turns out, it’s a path toward learning higher order representations of the source data. The filters are being trained to find patterns. The patterns are revealed in relationships between words and their neighbors! Just the kind of subtle   information you set out to find.\n",
        "\n",
        "In image processing, the first layers will tend to learn to be edge detectors, places where pixel densities rapidly shift from one side to the other. Later layers learn concepts like shape and texture. And layers after that may learn “content” or “meaning.” Similar processes will happen with text.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/pooling-layers.PNG?raw=1' width='800'/>\n",
        "\n",
        "You have two choices for pooling:\n",
        "* Average Pooling: Average is the more intuitive of the two in that by taking the average of the subset of values you would in theory retain the most data.\n",
        "* Max Pooling: has an interesting property, in that by taking the largest activation value for the given region, the network sees\n",
        "that subsection’s most prominent feature. The network has a path toward learning\n",
        "what it should look at, regardless of exact pixel-level position!\n",
        "\n",
        "In addition to dimensionality reduction and the computational savings that come\n",
        "with it, you gain something else special: **location invariance**. If an original input element is jostled slightly in position in a similar but distinct input sample, the max pooling layer will still output something similar. This is a huge boon in the image recognition world, and it serves a similar purpose in natural language processing.\n",
        "\n",
        "In Keras, you’re using the GlobalMaxPooling1D layer.\n",
        "\n",
        "```python\n",
        "model.add(GlobalMaxPooling1D())\n",
        "```\n",
        "Now for each input sample you have a 1D vector that the network thinks is a good representation of that input sample. This is a semantic representation of the input—a crude one to be sure. And it will only be semantic in the context of the training target, which is sentiment. There won’t be an encoding of the content of the movie being reviewed, say, just an encoding of its sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-Gqkg8I0B_b",
        "colab_type": "text"
      },
      "source": [
        "### Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdBO-Z8T0DI3",
        "colab_type": "text"
      },
      "source": [
        "Dropout is a special technique developed to prevent overfitting in neural networks. It isn’t specific to natural language processing, but it does work well here.\n",
        "\n",
        "The idea is that on each training pass, if you “turn off” a certain percentage of the input going to the next layer, randomly chosen on each pass, the model will be less likely to learn the specifics of the training set, “overfitting,” and instead learn more nuanced representations of the patterns in the data and thereby be able to generalize and make accurate predictions when it sees completely novel data.\n",
        "\n",
        "The parameter passed into the Dropout layer in Keras is the percentage of the inputs to randomly turn off. In this example, only 80% of the embedding data, randomly chosen for each training sample, will pass into the next layer as it is. The rest will go in as 0s. A 20% dropout setting is common, but a dropout of up to 50% can have good results.\n",
        "\n",
        "```python\n",
        "# You start with a vanilla fully connected hidden layer and then tack on dropout and ReLU.\n",
        "model.add(Dense(hidden_dims))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkF0zyt4s6IC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cfb9e556-5431-4931-8e99-eceed55453a7"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# we add a Convolution1D, which will learn filters word group filters of size filter_length\n",
        "model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1, input_shape=(maxlen, embedding_dims)))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "# vanilla hidden layer\n",
        "model.add(Dense(hidden_dims))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5aPTrww8HZ3",
        "colab_type": "text"
      },
      "source": [
        "You would like to save the model state after training.\n",
        "Because you aren’t going to hold the model in memory for now, you can grab its\n",
        "structure in a JSON file and save the trained weights in another file for later reinstantiation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viQIQimB8enz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_structure = model.to_json()   # Note that this doesn’t save the weights of the network, only the structure.\n",
        "\n",
        "# Save your trained model before you lose it!\n",
        "with open('cnn_model.json', 'w') as json_file:\n",
        "  json_file.write(model_structure)\n",
        "model.save_weights('cnn_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhheVs8B-k6A",
        "colab_type": "text"
      },
      "source": [
        "Now your trained model will be persisted on disk; should it converge, you won’t have to train it again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpssFGkjFrIi",
        "colab_type": "text"
      },
      "source": [
        "## Loading saved model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0-q-H2FFuXl",
        "colab_type": "text"
      },
      "source": [
        "After you have a trained model, you can then pass in a novel sample and see what the network thinks. This could be an incoming chat message or tweet to your bot; in your case, it’ll be a made-up example.\n",
        "\n",
        "First, reinstate your trained model, if it’s no longer in memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vT0JsbEF3Ou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tf.keras.models import model_from_json\n",
        "\n",
        "with open('cnn_model.json', 'r') as json_file:\n",
        "  json_string = json_file.read()\n",
        "\n",
        "model = model_from_json(json_string)\n",
        "model.load_weights('cnn_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5ccRG3FOg_j",
        "colab_type": "text"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ko8dLd8N0Mr",
        "colab_type": "text"
      },
      "source": [
        "Let’s make up a sentence with an obvious negative sentiment and see what the network has to say about it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDoFDTjjN1st",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_1 = \"\"\"\n",
        "I'm hate that the dismal weather that had me down for so long, when will it break! Ugh, when does happiness return?  \n",
        "The sun is blinding and the puffy clouds are too thin.  I can't wait for the weekend.\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ2hHRP4OKhG",
        "colab_type": "text"
      },
      "source": [
        "With the model pretrained, testing a new sample is quick. The are still thousands and\n",
        "thousands of calculations to do, but for each sample you only need one forward pass\n",
        "and no backpropagation to get a result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chBtRjF7OAlw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You pass a dummy value in the first element of the tuple just because\n",
        "# your helper expects it from the way you processed the initial data.\n",
        "# That value won’t ever see the network, so it can be anything.\n",
        "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
        "\n",
        "# Tokenize returns a list of the data (length 1 here)\n",
        "test_vec_list = pad_trunc(vec_list, maxlen)\n",
        "\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "model.predict(test_vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTHN4qssPkN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.predict_classes(test_vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d4_Zx0hQP8Y",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtKEj7dPQVSd",
        "colab_type": "text"
      },
      "source": [
        "We touched briefly on the output of the convolutional layers (before you step into\n",
        "the feedforward layer). This semantic representation is an important artifact. It’s in many\n",
        "ways a numerical representation of the thought and details of the input text. Specifically\n",
        "in this case, it’s a representation of the thought and details through the lens of sentiment\n",
        "analysis, as all the “learning” that happened was in response to whether the\n",
        "sample was labeled as a positive or negative sentiment. The vector that was generated\n",
        "by training on a set that was labeled for another specific topic and classified as such\n",
        "would contain much different information. Using the intermediary vector directly\n",
        "from a convolutional neural net isn’t common, but other neural network architectures where the details of that intermediary\n",
        "vector become important, and in some cases are the end goal itself.\n",
        "\n",
        "Why would you choose a CNN for your NLP classification task? The main benefit it\n",
        "provides is efficiency. In many ways, because of the pooling layers and the limits created\n",
        "by filter size (though you can make your filters large if you wish), you’re throwing\n",
        "away a good deal of information. But that doesn’t mean they aren’t useful models. As\n",
        "you’ve seen, they were able to efficiently detect and predict sentiment over a relatively\n",
        "large dataset, and even though you relied on the Word2vec embeddings, CNNs can\n",
        "perform on much less rich embeddings without mapping the entire language.\n",
        "\n"
      ]
    }
  ]
}