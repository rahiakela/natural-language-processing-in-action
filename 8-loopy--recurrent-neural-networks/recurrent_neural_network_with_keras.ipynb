{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "recurrent_neural_network_with_keras.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-in-action/blob/8-loopy-recurrent-neural-networks/recurrent_neural_network_with_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO1preWdp2QJ",
        "colab_type": "text"
      },
      "source": [
        "# Recurrent Neural Network with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfJtCbhbqTT1",
        "colab_type": "text"
      },
      "source": [
        "First, you load the dataset, grab the labels, and shuffle the examples. Then you\n",
        "tokenize it and vectorize it again using the Google Word2vec model. Next, you grab the labels. And finally you split it 80/20 into the training and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8dMuJFaqjr2",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shf2mXZKrZg4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f291ad5d-54e5-47ad-bc6b-bb0f16a825c7"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import backend as keras_backend\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, SimpleRNN\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "import re\n",
        "import tqdm\n",
        "\n",
        "import glob\n",
        "from random import shuffle\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "import requests"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woc_3WGcp50m",
        "colab_type": "code",
        "outputId": "61930f23-0907-455e-9c50-49bc256688e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! pip install pugnlp"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pugnlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c6/17a0ef5af34e20b595f1983be0468efa9f903a17a5af2a0a980ecaf9c411/pugnlp-0.2.5-py2.py3-none-any.whl (706kB)\n",
            "\u001b[K     |████████████████████████████████| 716kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /tensorflow-2.1.0/python3.6 (from pugnlp) (1.4.1)\n",
            "Collecting python-Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from pugnlp) (3.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pugnlp) (0.25.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from pugnlp) (3.2.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from pugnlp) (4.0.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from pugnlp) (4.4.1)\n",
            "Collecting pypandoc\n",
            "  Downloading https://files.pythonhosted.org/packages/71/81/00184643e5a10a456b4118fc12c96780823adb8ed974eb2289f29703b29b/pypandoc-1.4.tar.gz\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading https://files.pythonhosted.org/packages/d8/f1/5a267addb30ab7eaa1beab2b9323073815da4551076554ecc890a3595ec9/fuzzywuzzy-0.17.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pugnlp) (4.28.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pugnlp) (0.22.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from pugnlp) (0.9.0)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from pugnlp) (1.0.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (from pugnlp) (19.3.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pugnlp) (0.16.0)\n",
            "Requirement already satisfied: wheel in /tensorflow-2.1.0/python3.6 (from pugnlp) (0.33.6)\n",
            "Requirement already satisfied: coverage in /usr/local/lib/python3.6/dist-packages (from pugnlp) (3.7.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pugnlp) (3.1.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /tensorflow-2.1.0/python3.6 (from scipy->pugnlp) (1.18.1)\n",
            "Requirement already satisfied: setuptools in /tensorflow-2.1.0/python3.6 (from python-Levenshtein->pugnlp) (45.0.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /tensorflow-2.1.0/python3.6 (from gensim->pugnlp) (1.14.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->pugnlp) (1.9.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->pugnlp) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->pugnlp) (2.6.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->pugnlp) (1.3)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->pugnlp) (1.3.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pugnlp) (0.14.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->pugnlp) (4.6.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->pugnlp) (7.5.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->pugnlp) (5.2.2)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->pugnlp) (5.2.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->pugnlp) (5.6.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->pugnlp) (4.6.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pugnlp) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pugnlp) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pugnlp) (1.1.0)\n",
            "Requirement already satisfied: requests in /tensorflow-2.1.0/python3.6 (from smart-open>=1.2.1->gensim->pugnlp) (2.22.0)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->pugnlp) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->pugnlp) (1.10.47)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->pugnlp) (5.3.4)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->pugnlp) (4.3.3)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->pugnlp) (4.5.3)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->pugnlp) (5.5.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->pugnlp) (5.0.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->pugnlp) (3.5.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->pugnlp) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->pugnlp) (4.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->pugnlp) (2.10.3)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->pugnlp) (0.8.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->pugnlp) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->pugnlp) (2.1.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->pugnlp) (0.4.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->pugnlp) (3.1.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->pugnlp) (0.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->pugnlp) (1.4.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->pugnlp) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->pugnlp) (0.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /tensorflow-2.1.0/python3.6 (from requests->smart-open>=1.2.1->gensim->pugnlp) (2019.11.28)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /tensorflow-2.1.0/python3.6 (from requests->smart-open>=1.2.1->gensim->pugnlp) (1.25.7)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /tensorflow-2.1.0/python3.6 (from requests->smart-open>=1.2.1->gensim->pugnlp) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /tensorflow-2.1.0/python3.6 (from requests->smart-open>=1.2.1->gensim->pugnlp) (2.8)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->pugnlp) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->pugnlp) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->pugnlp) (1.13.47)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->jupyter->pugnlp) (17.0.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1.0->ipykernel->jupyter->pugnlp) (4.4.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->pugnlp) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->pugnlp) (4.7.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->pugnlp) (0.7.5)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->pugnlp) (2.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->jupyter->pugnlp) (1.1.1)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->pugnlp) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->pugnlp) (0.1.8)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->pugnlp) (0.5.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->smart-open>=1.2.1->gensim->pugnlp) (0.15.2)\n",
            "Building wheels for collected packages: python-Levenshtein, pypandoc\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144673 sha256=37e973b83ad5a326efc9d06a2282bff5cb40537343a442ccb9295c732bbb3782\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "  Building wheel for pypandoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypandoc: filename=pypandoc-1.4-cp36-none-any.whl size=16716 sha256=8312a1f77e614e1f906ef2b09bcbdf741536b76a82697419f15c5c5fdecb14ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/55/4f/59e0fa0914f3db52e87c0642c5fb986871dfbbf253026e639f\n",
            "Successfully built python-Levenshtein pypandoc\n",
            "Installing collected packages: python-Levenshtein, pypandoc, fuzzywuzzy, pugnlp\n",
            "Successfully installed fuzzywuzzy-0.17.0 pugnlp-0.2.5 pypandoc-1.4 python-Levenshtein-0.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evwKZIKdp6h3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pugnlp.futil import path_status, find_files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swxx7Tmxr0np",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQymxGOrr3X5",
        "colab_type": "text"
      },
      "source": [
        "Each data point is prelabeled with a 0 (negative sentiment) or a 1 (positive sentiment).you’re going to swap out their example IMDB movie review dataset\n",
        "for one in raw text, so you can get your hands dirty with the preprocessing of the text as well. And then you’ll see if you can use this trained network to classify text it has never seen before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8RMv-lktomF",
        "colab_type": "text"
      },
      "source": [
        "### Downloading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1jLX7hOrsr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BIG_URLS = {\n",
        "    'w2v': ('https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1', 1647046227),\n",
        "    'slang': ('https://www.dropbox.com/s/43c22018fbfzypd/slang.csv.gz?dl=1', 117633024),\n",
        "    'tweets': ('https://www.dropbox.com/s/5gpb43c494mc8p0/tweets.csv.gz?dl=1', 311725313),\n",
        "    'lsa_tweets': ('https://www.dropbox.com/s/rpjt0d060t4n1mr/lsa_tweets_5589798_2003588x200.tar.gz?dl=1', 3112841563),  # 3112841312\n",
        "    'imdb': ('https://www.dropbox.com/s/yviic64qv84x73j/aclImdb_v1.tar.gz?dl=1', 3112841563),  # 3112841312\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHaBlbdwtXG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These functions are part of the nlpia package which can be pip installed and run from there.\n",
        "def dropbox_basename(url):\n",
        "    filename = os.path.basename(url)\n",
        "    match = re.findall(r'\\?dl=[0-9]$', filename)\n",
        "    if match:\n",
        "        return filename[:-len(match[0])]\n",
        "    return filename\n",
        "\n",
        "def download_file(url, data_path='.', filename=None, size=None, chunk_size=4096, verbose=True):\n",
        "    \"\"\"Uses stream=True and a reasonable chunk size to be able to download large (GB) files over https\"\"\"\n",
        "    if filename is None:\n",
        "        filename = dropbox_basename(url)\n",
        "    file_path = os.path.join(data_path, filename)\n",
        "    if url.endswith('?dl=0'):\n",
        "        url = url[:-1] + '1'  # noninteractive download\n",
        "    if verbose:\n",
        "        tqdm_prog = tqdm\n",
        "        print('requesting URL: {}'.format(url))\n",
        "    else:\n",
        "        tqdm_prog = no_tqdm\n",
        "    r = requests.get(url, stream=True, allow_redirects=True)\n",
        "    size = r.headers.get('Content-Length', None) if size is None else size\n",
        "    print('remote size: {}'.format(size))\n",
        "\n",
        "    stat = path_status(file_path)\n",
        "    print('local size: {}'.format(stat.get('size', None)))\n",
        "    if stat['type'] == 'file' and stat['size'] == size:  # TODO: check md5 or get the right size of remote file\n",
        "        r.close()\n",
        "        return file_path\n",
        "\n",
        "    print('Downloading to {}'.format(file_path))\n",
        "\n",
        "    with open(file_path, 'wb') as f:\n",
        "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "            if chunk:  # filter out keep-alive chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "    r.close()\n",
        "    return file_path\n",
        "\n",
        "def untar(fname):\n",
        "    if fname.endswith(\"tar.gz\"):\n",
        "        with tarfile.open(fname) as tf:\n",
        "            tf.extractall()\n",
        "    else:\n",
        "        print(\"Not a tar.gz file: {}\".format(fname))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0oWrTZ1uFHh",
        "colab_type": "code",
        "outputId": "8508a675-4eb9-4c86-94c6-519d427e7063",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "download_file(BIG_URLS['w2v'][0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "requesting URL: https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1\n",
            "remote size: 1647046227\n",
            "local size: None\n",
            "Downloading to ./GoogleNews-vectors-negative300.bin.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./GoogleNews-vectors-negative300.bin.gz'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I09TKCmzuF6_",
        "colab_type": "code",
        "outputId": "c0a7b24e-f832-4356-da93-9c3c975478be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "untar(download_file(BIG_URLS['imdb'][0]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "requesting URL: https://www.dropbox.com/s/yviic64qv84x73j/aclImdb_v1.tar.gz?dl=1\n",
            "remote size: 84125825\n",
            "local size: None\n",
            "Downloading to ./aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipv8IOS3v6AK",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing the loaded documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjVYBXqrwT7V",
        "colab_type": "text"
      },
      "source": [
        "The reviews in the train folder are broken up into text files in either the pos or neg folders. You’ll first need to read those in Python with their appropriate label and then shuffle the deck so the samples aren’t all positive and then all negative. Training with the sorted labels will skew training toward whatever comes last, especially when you use certain hyperparameters, such as momentum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtmvAhqfv5Mo",
        "colab_type": "code",
        "outputId": "b8cb3312-437d-4e5f-e0e9-28ccce4e6835",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "import glob\n",
        "from random import shuffle\n",
        "\n",
        "def pre_process_data(filepath):\n",
        "  '''\n",
        "  This is dependent on your training data source but we will try to generalize it as best as possible.\n",
        "  '''\n",
        "  positive_path = os.path.join(filepath, 'pos')\n",
        "  negative_path = os.path.join(filepath, 'neg')\n",
        "\n",
        "  pos_label = 1\n",
        "  neg_label = 0\n",
        "\n",
        "  dataset = []\n",
        "\n",
        "  for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
        "    with open(filename, 'r') as f:\n",
        "      dataset.append((pos_label, f.read()))\n",
        "\n",
        "  for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
        "    with open(filename, 'r') as f:\n",
        "      dataset.append((neg_label, f.read()))\n",
        "\n",
        "  shuffle(dataset)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "dataset = pre_process_data('./aclImdb/train')\n",
        "print(dataset[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 'This movie of 370 minutes was aired by the Italian public television during the early seventies. It tells you the myth attributed to Homer of the Journey home of Odysseus after the Troy war. It is an epic story about the ancient Minoan and Mycenaean civilizations, told at list 500 years after those events toke place, around 1100 BC.<br /><br />This is a 1969 movie, so if you buy the DVD version you would find that the sound is just mono and there is no other language than Italian, even the close caption is in Italian. Pity. Many people would enjoy this masterpiece if it had at list the English subtitles. But if this is not a problem for you, than I would strongly recommend to watch this movie.')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W76Jm0mbGm3o",
        "colab_type": "text"
      },
      "source": [
        "### Data tokenization and vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT3S4Gd3FCvO",
        "colab_type": "text"
      },
      "source": [
        "The next step is to tokenize and vectorize the data. You’ll use the Google News pretrained Word2vec vectors, so download those directly from Google.\n",
        "\n",
        "You’ll use gensim to unpack the vectors, You can\n",
        "experiment with the limit argument to the load_word2vec_format method; a\n",
        "higher number will get you more vectors to play with, but memory quickly becomes an issue and return on investment drops quickly in really high values for limit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slErmWLvxsca",
        "colab_type": "code",
        "outputId": "47885aa9-47b6-49cf-ecf2-fe8f096558ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True, limit=200000)\n",
        "\n",
        "def tokenize_and_vectorize(dataset):\n",
        "  tokenizer = TreebankWordTokenizer()\n",
        "  vectorized_data = []\n",
        "  expected = []\n",
        "\n",
        "  for sample in dataset:\n",
        "    tokens = tokenizer.tokenize(sample[1])\n",
        "    sample_vecs = []\n",
        "    for token in tokens:\n",
        "      try:\n",
        "        sample_vecs.append(word_vectors[token])\n",
        "      except KeyError:\n",
        "        pass    # No matching token in the Google w2v vocab\n",
        "\n",
        "    vectorized_data.append(sample_vecs)\n",
        "\n",
        "  return vectorized_data"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGF3PpBhpe11",
        "colab_type": "code",
        "outputId": "ff590de5-9bec-4c79-fdd5-fa4ef2cae09a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "word_vectors['dog']"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5.12695312e-02, -2.23388672e-02, -1.72851562e-01,  1.61132812e-01,\n",
              "       -8.44726562e-02,  5.73730469e-02,  5.85937500e-02, -8.25195312e-02,\n",
              "       -1.53808594e-02, -6.34765625e-02,  1.79687500e-01, -4.23828125e-01,\n",
              "       -2.25830078e-02, -1.66015625e-01, -2.51464844e-02,  1.07421875e-01,\n",
              "       -1.99218750e-01,  1.59179688e-01, -1.87500000e-01, -1.20117188e-01,\n",
              "        1.55273438e-01, -9.91210938e-02,  1.42578125e-01, -1.64062500e-01,\n",
              "       -8.93554688e-02,  2.00195312e-01, -1.49414062e-01,  3.20312500e-01,\n",
              "        3.28125000e-01,  2.44140625e-02, -9.71679688e-02, -8.20312500e-02,\n",
              "       -3.63769531e-02, -8.59375000e-02, -9.86328125e-02,  7.78198242e-03,\n",
              "       -1.34277344e-02,  5.27343750e-02,  1.48437500e-01,  3.33984375e-01,\n",
              "        1.66015625e-02, -2.12890625e-01, -1.50756836e-02,  5.24902344e-02,\n",
              "       -1.07421875e-01, -8.88671875e-02,  2.49023438e-01, -7.03125000e-02,\n",
              "       -1.59912109e-02,  7.56835938e-02, -7.03125000e-02,  1.19140625e-01,\n",
              "        2.29492188e-01,  1.41601562e-02,  1.15234375e-01,  7.50732422e-03,\n",
              "        2.75390625e-01, -2.44140625e-01,  2.96875000e-01,  3.49121094e-02,\n",
              "        2.42187500e-01,  1.35742188e-01,  1.42578125e-01,  1.75781250e-02,\n",
              "        2.92968750e-02, -1.21582031e-01,  2.28271484e-02, -4.76074219e-02,\n",
              "       -1.55273438e-01,  3.14331055e-03,  3.45703125e-01,  1.22558594e-01,\n",
              "       -1.95312500e-01,  8.10546875e-02, -6.83593750e-02, -1.47094727e-02,\n",
              "        2.14843750e-01, -1.21093750e-01,  1.57226562e-01, -2.07031250e-01,\n",
              "        1.36718750e-01, -1.29882812e-01,  5.29785156e-02, -2.71484375e-01,\n",
              "       -2.98828125e-01, -1.84570312e-01, -2.29492188e-01,  1.19140625e-01,\n",
              "        1.53198242e-02, -2.61718750e-01, -1.23046875e-01, -1.86767578e-02,\n",
              "       -6.49414062e-02, -8.15429688e-02,  7.86132812e-02, -3.53515625e-01,\n",
              "        5.24902344e-02, -2.45361328e-02, -5.43212891e-03, -2.08984375e-01,\n",
              "       -2.10937500e-01, -1.79687500e-01,  2.42187500e-01,  2.57812500e-01,\n",
              "        1.37695312e-01, -2.10937500e-01, -2.17285156e-02, -1.38671875e-01,\n",
              "        1.84326172e-02, -1.23901367e-02, -1.59179688e-01,  1.61132812e-01,\n",
              "        2.08007812e-01,  1.03027344e-01,  9.81445312e-02, -6.83593750e-02,\n",
              "       -8.72802734e-03, -2.89062500e-01, -2.14843750e-01, -1.14257812e-01,\n",
              "       -2.21679688e-01,  4.12597656e-02, -3.12500000e-01, -5.59082031e-02,\n",
              "       -9.76562500e-02,  5.81054688e-02, -4.05273438e-02, -1.73828125e-01,\n",
              "        1.64062500e-01, -2.53906250e-01, -1.54296875e-01, -2.31933594e-02,\n",
              "       -2.38281250e-01,  2.07519531e-02, -2.73437500e-01,  3.90625000e-03,\n",
              "        1.13769531e-01, -1.73828125e-01,  2.57812500e-01,  2.35351562e-01,\n",
              "        5.22460938e-02,  6.83593750e-02, -1.75781250e-01,  1.60156250e-01,\n",
              "       -5.98907471e-04,  5.98144531e-02, -2.11914062e-01, -5.54199219e-02,\n",
              "       -7.51953125e-02, -3.06640625e-01,  4.27734375e-01,  5.32226562e-02,\n",
              "       -2.08984375e-01, -5.71289062e-02, -2.09960938e-01,  3.29589844e-02,\n",
              "        1.05468750e-01, -1.50390625e-01, -9.37500000e-02,  1.16699219e-01,\n",
              "        6.44531250e-02,  2.80761719e-02,  2.41210938e-01, -1.25976562e-01,\n",
              "       -1.00585938e-01, -1.22680664e-02, -3.26156616e-04,  1.58691406e-02,\n",
              "        1.27929688e-01, -3.32031250e-02,  4.07714844e-02, -1.31835938e-01,\n",
              "        9.81445312e-02,  1.74804688e-01, -2.36328125e-01,  5.17578125e-02,\n",
              "        1.83593750e-01,  2.42919922e-02, -4.31640625e-01,  2.46093750e-01,\n",
              "       -3.03955078e-02, -2.47802734e-02, -1.17187500e-01,  1.61132812e-01,\n",
              "       -5.71289062e-02,  1.16577148e-02,  2.81250000e-01,  4.27734375e-01,\n",
              "        4.56542969e-02,  1.01074219e-01, -3.95507812e-02,  1.77001953e-02,\n",
              "       -8.98437500e-02,  1.35742188e-01,  2.08007812e-01,  1.88476562e-01,\n",
              "       -1.52343750e-01, -2.37304688e-01, -1.90429688e-01,  7.12890625e-02,\n",
              "       -2.46093750e-01, -2.61718750e-01, -2.34375000e-01, -1.45507812e-01,\n",
              "       -1.17187500e-02, -1.50390625e-01, -1.13281250e-01,  1.82617188e-01,\n",
              "        2.63671875e-01, -1.37695312e-01, -4.58984375e-01, -4.68750000e-02,\n",
              "       -1.26953125e-01, -4.22363281e-02, -1.66992188e-01,  1.26953125e-01,\n",
              "        2.59765625e-01, -2.44140625e-01, -2.19726562e-01, -8.69140625e-02,\n",
              "        1.59179688e-01, -3.78417969e-02,  8.97216797e-03, -2.77343750e-01,\n",
              "       -1.04980469e-01, -1.75781250e-01,  2.28515625e-01, -2.70996094e-02,\n",
              "        2.85156250e-01, -2.73437500e-01,  1.61132812e-02,  5.90820312e-02,\n",
              "       -2.39257812e-01,  1.77734375e-01, -1.34765625e-01,  1.38671875e-01,\n",
              "        3.53515625e-01,  1.22070312e-01,  1.43554688e-01,  9.22851562e-02,\n",
              "        2.29492188e-01, -3.00781250e-01, -4.88281250e-02, -1.79687500e-01,\n",
              "        2.96875000e-01,  1.75781250e-01,  4.80957031e-02, -3.38745117e-03,\n",
              "        7.91015625e-02, -2.38281250e-01, -2.31445312e-01,  1.66015625e-01,\n",
              "       -2.13867188e-01, -7.03125000e-02, -7.56835938e-02,  1.96289062e-01,\n",
              "       -1.29882812e-01, -1.05957031e-01, -3.53515625e-01, -1.16699219e-01,\n",
              "       -5.10253906e-02,  3.39355469e-02, -1.43554688e-01, -3.90625000e-03,\n",
              "        1.73828125e-01, -9.96093750e-02, -1.66015625e-01, -8.54492188e-02,\n",
              "       -3.82812500e-01,  5.90820312e-02, -6.22558594e-02,  8.83789062e-02,\n",
              "       -8.88671875e-02,  3.28125000e-01,  6.83593750e-02, -1.91406250e-01,\n",
              "       -8.35418701e-04,  1.04003906e-01,  1.52343750e-01, -1.53350830e-03,\n",
              "        4.16015625e-01, -3.32031250e-02,  1.49414062e-01,  2.42187500e-01,\n",
              "       -1.76757812e-01, -4.93164062e-02, -1.24511719e-01,  1.25976562e-01,\n",
              "        1.74804688e-01,  2.81250000e-01, -1.80664062e-01,  1.03027344e-01,\n",
              "       -2.75390625e-01,  2.61718750e-01,  2.46093750e-01, -4.71191406e-02,\n",
              "        6.25000000e-02,  4.16015625e-01, -3.55468750e-01,  2.22656250e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1Z3bpwfIfyq",
        "colab_type": "text"
      },
      "source": [
        "You also need to collect the target values—0 for a negative review, 1 for a positive review—in the same order as the training samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC1qJoaNH5gF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_expected(dataset):\n",
        "  '''Peel of the target values from the dataset'''\n",
        "  expected = []\n",
        "  for sample in dataset:\n",
        "    expected.append(sample[0])\n",
        "  \n",
        "  return expected"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAwhZvflImSK",
        "colab_type": "text"
      },
      "source": [
        "And then you simply pass your data into those functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu5lfSiGIMwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorized_data = tokenize_and_vectorize(dataset)\n",
        "expected = collect_expected(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u65UIysaI_cP",
        "colab_type": "text"
      },
      "source": [
        "### Train/Test splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EgeyjZgJHu4",
        "colab_type": "text"
      },
      "source": [
        "Next you’ll split the prepared data into a training set and a test set. You’re just going to split your imported dataset 80/20, but this ignores the folder of test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuO_XoY4JQd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_point = int(len(vectorized_data) * .8)\n",
        "\n",
        "x_train = vectorized_data[:split_point]\n",
        "y_train = expected[:split_point]\n",
        "x_test = vectorized_data[split_point:]\n",
        "y_test = expected[split_point:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERwTnAjrKccV",
        "colab_type": "text"
      },
      "source": [
        "### Hyper-parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ_9-wsmKReT",
        "colab_type": "text"
      },
      "source": [
        "The next sets most of the hyperparameters for the net."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2Y0InHSKqGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen = 400          # holds the maximum review length\n",
        "batch_size = 32       # How many samples to show the net before backpropagating the error and updating the weights\n",
        "embedding_dims = 300  # Length of the token vectors you’ll create for passing into the convnet\n",
        "epochs = 2            # Number of times we will pass the entire training dataset through the network"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qAveehRMnBK",
        "colab_type": "text"
      },
      "source": [
        "### Padding and truncating token sequence(sequences of vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di-Sa4YOMshT",
        "colab_type": "text"
      },
      "source": [
        "Keras has a preprocessing helper method, pad_sequences, that in theory could be\n",
        "used to pad your input data, but unfortunately it works only with sequences of scalars, and you have sequences of vectors. \n",
        "\n",
        "Let’s write a helper function of your own to pad your input data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v51zAgevM0Kq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_trunc(data, maxlen):\n",
        "  '''For a given dataset pad with zero vectors or truncate to maxlen'''\n",
        "  new_data = []\n",
        "\n",
        "  # Create a vector of 0's the length of our word vectors\n",
        "  zero_vector = []\n",
        "  for _ in range(len(data[0][0])):\n",
        "    zero_vector.append(0.0)\n",
        "  #zero_vector = [0.0 for _ in range(len(data[0][0]))]\n",
        "\n",
        "  for sample in data:\n",
        "    if len(sample) > maxlen:\n",
        "        temp = sample[:maxlen]\n",
        "    elif len(sample) < maxlen:\n",
        "        temp = sample\n",
        "        additional_elems = maxlen - len(sample)\n",
        "        for _ in range(additional_elems):\n",
        "            temp.append(zero_vector)\n",
        "    else:\n",
        "        temp = sample\n",
        "    new_data.append(temp)\n",
        "  \n",
        "  return new_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKWpu1TYODk2",
        "colab_type": "text"
      },
      "source": [
        "Then you need to pass your train and test data into the padder/truncator. After that you can convert it to numpy arrays to make Keras happy. This is a tensor with the shape (number of samples, sequence length, word vector length) that you need for your CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGkviQPlFb_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = pad_trunc(x_train, maxlen)\n",
        "x_test = pad_trunc(x_test, maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6qc3GwKNxGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoHKIV9yrQTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras_backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mj96pjMOvxX",
        "colab_type": "text"
      },
      "source": [
        "Phew; finally you’re ready to build a neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8CB1D4XOwQm",
        "colab_type": "text"
      },
      "source": [
        "## Recurrent neural network architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ildkqgWOyny",
        "colab_type": "text"
      },
      "source": [
        "Sequential is one of the base classes for neural networks in Keras. From here you can start to layer on the magic.\n",
        "\n",
        "```python\n",
        "model = Sequential()\n",
        "```\n",
        "And then, as before, the Keras magic handles the complexity of assembling a neural net: you just need to add the recurrent layer you want to your network.\n",
        "\n",
        "```python\n",
        "num_neurons = 50\n",
        "odel.add(SimpleRNN(num_neurons, return_sequences=True,input_shape=(maxlen,embedding_dims)))\n",
        "```\n",
        "\n",
        "Now the infrastructure is set up to take each input and pass it into a simple recurrent neural net and for each token, gather\n",
        "the output into a vector. Because your sequences are 400 tokens long and you’re using 50 hidden neurons, your output from this layer will be a vector 400 elements long. Each of those elements is a vector 50 elements long, with one output for each of the neurons.\n",
        "\n",
        "Notice here the keyword argument return_sequences. It’s going to tell the network to return the network value at each time step, hence the 400 vectors, each 50 long. If return_sequences was set to False (the Keras default behavior), only a single 50-dimensional vector would be returned.\n",
        "\n",
        "\n",
        "When using a recurrent neural net, truncating and padding isn’t usually necessary. You can provide training data of varying lengths and unroll the net until you hit the end of the input. Keras will handle this automatically. The catch is that your output of the recurrent layer will vary from time step to time step with the input. A four-token input will output a sequence four elements long. A 100-token sequence will produce a sequence of 100 elements. If you need to pass this into another layer, one that expects a uniform input, it won’t work. But there are cases where that’s acceptable, and even preferred. But back to your classifier.\n",
        "\n",
        "\n",
        "```python\n",
        "model.add(Dropout(.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "```\n",
        "\n",
        "You requested that the simple RNN return full sequences, but to prevent overfitting you add a Dropout layer to zero out 20% of those inputs, randomly chosen on each input example. And then finally you add a classifier. In this case, you have one class: “Yes - Positive Sentiment - 1” or “No - Negative Sentiment - 0,” so you chose a layer with one neuron (Dense(1)) and a sigmoid activation function. \n",
        "\n",
        "But a Dense layer expects a “flat” vector of n elements (each element a float) as input. And the data coming out of the SimpleRNN is a tensor 400 elements long, and each of those are 50 elements long. But a feedforward network doesn’t care about order of elements as long as you’re consistent with the order. You use the convenience layer, Flatten(), that Keras provides to flatten the input from a 400 x 50 tensor to a vector 20,000 elements long. And that’s what you pass into the final layer that’ll make the classification.\n",
        "\n",
        "In reality, the Flatten layer is a mapping. That means the error is backpropagated from the last layer back to the appropriate output in the RNN layer and each of those backpropagated errors are then backpropagated through time from the appropriate point in the output.\n",
        "\n",
        "Passing the “thought vector” produced by the recurrent neural network layer into a feedforward network no longer keeps the order of the input you tried so hard to incorporate. But the important takeaway is to notice that the “learning” related to\n",
        "sequence of tokens happens in the RNN layer itself; the aggregation of errors via backpropagation through time is encoding that relationship in the network and expressing\n",
        "it in the “thought vector” itself. Your decision based on the thought vector, via the classifier, is providing feedback to the “quality” of that thought vector with respect to your specific classification problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JRIDUL3uZbW",
        "colab_type": "text"
      },
      "source": [
        "### Putting things together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgLDVCooUYXG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "81a9b082-bc96-42cc-e648-78065331607f"
      },
      "source": [
        "num_neurons = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn (SimpleRNN)       (None, 400, 50)           17550     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 400, 50)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 20000)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 20001     \n",
            "=================================================================\n",
            "Total params: 37,551\n",
            "Trainable params: 37,551\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S609TpBkWOEI",
        "colab_type": "text"
      },
      "source": [
        "In the SimpleRNN layer, you requested 50 neurons. Each of those neurons will\n",
        "receive input (and apply a weight to) each input sample. In an RNN, the input at each\n",
        "time step is one token. Your tokens are represented by word vectors in this case, each\n",
        "300 elements long (300-dimensional). Each neuron will need 300 weights:\n",
        "\n",
        "50 * 300 = 15,000\n",
        "\n",
        "Each neuron also has the bias term, which always has an input value of 1 (that’s what\n",
        "makes it a bias) but has a trainable weight:\n",
        "\n",
        "15,000 + 50 (bias weights) = 15,050\n",
        "\n",
        "15,050 weights in the first time step of the first layer. Now each of those 50 neurons\n",
        "will feed its output into the network’s next time step. Each neuron accepts the full\n",
        "input vector as well as the full output vector. In the first time step, the feedback from\n",
        "the output doesn’t exist yet. It’s initiated as a vector of zeros, its length the same as the\n",
        "length of the output.\n",
        "\n",
        "Each neuron in the hidden layer now has weights for each token embedding dimension:\n",
        "that’s 300 weights. It also has 1 bias for each neuron. And you have the 50 weights\n",
        "for the output results in the previous time step (or zeros for the first t=0 time step).\n",
        "These 50 weights are the key feedback step in a recurrent neural network. That gives us\n",
        "\n",
        "300 + 1 + 50 = 351\n",
        "\n",
        "351 times 50 neurons gives:\n",
        "\n",
        "351 * 50 = 17,550\n",
        "\n",
        "17,550 parameters to train. You’re unrolling this net 400 time steps (probably too much given the problems associated with vanishing gradients, but even so, this network turns out to still be effective). But those 17,550 parameters are the same in each of the unrollings, and they remain the same until all the backpropagations have been calculated.\n",
        "The updates to the weights occur at once at the end of the sequence forward propagation and subsequent backpropagation out to still be effective). But those 17,550 parameters are the same in each of the unrollings, and they remain the same until all the backpropagations have been calculated.\n",
        "The updates to the weights occur at once at the end of the sequence forward propagation and subsequent backpropagation Although you’re adding complexity to\n",
        "the backpropagation algorithm, you’re saved by the fact you’re not training a net with\n",
        "a little over 7 million parameters (17,550 * 400), which is what it would look like if the\n",
        "unrollings each had their own weight sets.\n",
        "\n",
        "The final layer in the summary is reporting 20,001 parameters to train, which is relatively straightforward. After the Flatten() layer, the input is a 20,000-dimensional vector plus the one bias input. Because you only have one neuron in the output layer, the total number of parameters is \n",
        "\n",
        "(20,000 input elements + 1 bias unit) * 1 neuron = 20,001 parameters\n",
        "\n",
        "Those numbers can be a little misleading in computational time because there are so many extra steps to backpropagation through time (compared to convolutional neural networks or standard feedforward networks). Computation time shouldn’t be a\n",
        "deal killer. Recurrent nets’ special talent at memory is the start of a bigger world in NLP or any other sequence data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-Gqkg8I0B_b",
        "colab_type": "text"
      },
      "source": [
        "### Traing and saving model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdBO-Z8T0DI3",
        "colab_type": "text"
      },
      "source": [
        "OK, now it’s time to actually train that recurrent network that we so carefully assembled\n",
        "in the previous section. As with your other Keras models, you need to give the\n",
        ".fit() method your data and tell it how long you want to run training (epochs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkF0zyt4s6IC",
        "colab_type": "code",
        "outputId": "a87ce9ff-8a10-4014-87c6-61e0b1345b4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        "# train the model\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/2\n",
            "20000/20000 [==============================] - 176s 9ms/sample - loss: 0.5819 - accuracy: 0.7114 - val_loss: 0.5115 - val_accuracy: 0.7654\n",
            "Epoch 2/2\n",
            "20000/20000 [==============================] - 172s 9ms/sample - loss: 0.4252 - accuracy: 0.8071 - val_loss: 0.4587 - val_accuracy: 0.8006\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8df42ba3c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5aPTrww8HZ3",
        "colab_type": "text"
      },
      "source": [
        "You would like to save the model state after training.\n",
        "Because you aren’t going to hold the model in memory for now, you can grab its\n",
        "structure in a JSON file and save the trained weights in another file for later reinstantiation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viQIQimB8enz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_structure = model.to_json()   # Note that this doesn’t save the weights of the network, only the structure.\n",
        "\n",
        "# Save your trained model before you lose it!\n",
        "with open('simple_rnn_model1.json', 'w') as json_file:\n",
        "  json_file.write(model_structure)\n",
        "model.save_weights('simple_rnn_weights1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhheVs8B-k6A",
        "colab_type": "text"
      },
      "source": [
        "Now your trained model will be persisted on disk; should it converge, you won’t have to train it again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5ccRG3FOg_j",
        "colab_type": "text"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ko8dLd8N0Mr",
        "colab_type": "text"
      },
      "source": [
        "Let’s make up a sentence with an obvious negative sentiment and see what the network has to say about it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfls5muqpnEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading model\n",
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "with open('simple_rnn_model1.json', 'r') as json_file:\n",
        "  json_string = json_file.read()\n",
        "model = model_from_json(json_string)\n",
        "\n",
        "model.load_weights('simple_rnn_weights1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDoFDTjjN1st",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_1 = \"\"\"\n",
        "I'm hate that the dismal weather that had me down for so long, when will it break! Ugh, when does happiness return?  \n",
        "The sun is blinding and the puffy clouds are too thin.  I can't wait for the weekend.\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ2hHRP4OKhG",
        "colab_type": "text"
      },
      "source": [
        "With the model pretrained, testing a new sample is quick. The are still thousands and\n",
        "thousands of calculations to do, but for each sample you only need one forward pass\n",
        "and no backpropagation to get a result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chBtRjF7OAlw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c9fc9a56-7049-441b-e0c4-cff4a9ee3a49"
      },
      "source": [
        "# You pass a dummy value in the first element of the tuple just because\n",
        "# your helper expects it from the way you processed the initial data.\n",
        "# That value won’t ever see the network, so it can be anything.\n",
        "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
        "\n",
        "# Tokenize returns a list of the data (length 1 here)\n",
        "test_vec_list = pad_trunc(vec_list, maxlen)\n",
        "\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "model.predict(test_vec)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.23188677]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTHN4qssPkN7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1a43bc6f-36ee-416a-ff55-82caaf37eea9"
      },
      "source": [
        "model.predict_classes(test_vec)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k99l4XH7sO4W",
        "colab_type": "text"
      },
      "source": [
        "### Build a larger network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDR7XE2rsRuB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "23534b34-8471-4a31-9bc9-81fd0315ffe7"
      },
      "source": [
        "num_neurons = 100\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(SimpleRNN(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims)))\n",
        "model1.add(Dropout(0.2))\n",
        "\n",
        "model1.add(Flatten())\n",
        "model1.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model1.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model1.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_2 (SimpleRNN)     (None, 400, 100)          40100     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 400, 100)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 40000)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 40001     \n",
            "=================================================================\n",
            "Total params: 80,101\n",
            "Trainable params: 80,101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un8caIkctTFp",
        "colab_type": "text"
      },
      "source": [
        "Train your larger network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZg2-d0XtT28",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "da96d2a1-b5f3-4f46-dc68-6b175248aa0e"
      },
      "source": [
        "model1.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/2\n",
            "20000/20000 [==============================] - 165s 8ms/sample - loss: 0.6884 - accuracy: 0.6708 - val_loss: 0.5532 - val_accuracy: 0.7420\n",
            "Epoch 2/2\n",
            "20000/20000 [==============================] - 165s 8ms/sample - loss: 0.4485 - accuracy: 0.8050 - val_loss: 0.5035 - val_accuracy: 0.7776\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8ddc3bb3c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkIvMh6bt6mT",
        "colab_type": "text"
      },
      "source": [
        "The validation accuracy of 78.24% is only 0.04% better after we doubled the complexity of our model in one of the layers. This negligible improvement should lead you to think the model (for this network layer) is too complex for the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NPNfAz0tfto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_structure = model1.to_json()   # Note that this doesn’t save the weights of the network, only the structure.\n",
        "\n",
        "# Save your trained model before you lose it!\n",
        "with open('simple_rnn_model2.json', 'w') as json_file:\n",
        "  json_file.write(model_structure)\n",
        "model1.save_weights('simple_rnn_weights2.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT--kwllu5-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading model\n",
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "with open('simple_rnn_model2.json', 'r') as json_file:\n",
        "  json_string = json_file.read()\n",
        "model = model_from_json(json_string)\n",
        "\n",
        "model.load_weights('simple_rnn_weights2.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpEQc5F7vAht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_1 = \"\"\"\n",
        "I'm hate that the dismal weather that had me down for so long, when will it break! Ugh, when does happiness return?  \n",
        "The sun is blinding and the puffy clouds are too thin.  I can't wait for the weekend.\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGyq5igPvFYP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3c655adb-779c-424b-b0c4-3fd4374267e6"
      },
      "source": [
        "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
        "\n",
        "# Tokenize returns a list of the data (length 1 here)\n",
        "test_vec_list = pad_trunc(vec_list, maxlen)\n",
        "\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "model.predict(test_vec)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.46618623]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ-Ki055vNha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1d514f89-a8dc-406c-bb8b-843086283f2c"
      },
      "source": [
        "model.predict_classes(test_vec)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLdYsuVPvnmf",
        "colab_type": "text"
      },
      "source": [
        "If you feel the model is overfitting the training data but you can’t find a way to make\n",
        "your model simpler, you can always try increasing the Dropout(percentage). This is\n",
        "a sledgehammer (actually a shotgun) that can mitigate the risk of overfitting while\n",
        "allowing your model to have as much complexity as it needs to match the data. If you\n",
        "set the dropout percentage much above 50%, the model starts to have a difficult time\n",
        "learning. Your learning will slow and validation error will bounce around a lot. But\n",
        "20% to 50% is a pretty safe range for a lot of NLP problems for recurrent networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d4_Zx0hQP8Y",
        "colab_type": "text"
      },
      "source": [
        "## Statefulness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtKEj7dPQVSd",
        "colab_type": "text"
      },
      "source": [
        "Sometimes you want to remember information from one input sample to the next, not just one-time step (token) to the next within a single sample.\n",
        "\n",
        "Keras provides a keyword argument in the base RNN layer called stateful. It defaults to False. If you flip this to True\n",
        "when adding the SimpleRNN layer to your model, the last sample’s last output passes\n",
        "into itself at the next time step along with the first token input, just as it would in the\n",
        "middle of the sample.\n",
        "\n",
        "Setting stateful to True can be a good idea when you want to model a large document\n",
        "that has been split into paragraphs or sentences for processing. And you might even use it to model the meaning of an entire corpus of related documents. But you\n",
        "wouldn’t want to train a stateful RNN on unrelated documents or passages without\n",
        "resetting the state of the model between samples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV-z-e_WwyUb",
        "colab_type": "text"
      },
      "source": [
        "## Two-way street"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1tZQpqpwzd9",
        "colab_type": "text"
      },
      "source": [
        "So far we’ve discussed relationships between words and what has come before. But\n",
        "can’t a case be made for flipping those word dependencies?\n",
        "\n",
        "*They wanted to pet the dog whose fur was brown.*\n",
        "\n",
        "As you get to the token “fur,” you have encountered “dog” already and know something\n",
        "about it. But the sentence also contains the information that the dog has fur,\n",
        "and that the dog’s fur is brown. And that information is relevant to the previous action\n",
        "of petting and the fact that “they” wanted to do the petting. Perhaps “they” only like to\n",
        "pet soft, furry brown things and don’t like petting prickly green things like cacti.\n",
        "\n",
        "Humans read the sentence in one direction but are capable of flitting back to earlier\n",
        "parts of the text in their brain as new information is revealed. Humans can deal\n",
        "with information that isn’t presented in the best possible order. It would be nice if you\n",
        "could allow your model to flit back across the input as well. That is where bidirectional\n",
        "recurrent neural nets come in.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/bidirectional-recurrent-neural-net.png?raw=1' width='800'/>\n",
        "\n",
        "The basic idea is you arrange two RNNs right next to each other, passing the input into one as normal and the same input backward into the other net.\n",
        "\n",
        "The output of those two are then concatenated at each time step to the related (same input token) time step in the other network. You take the output of the final time step\n",
        "in the input and concatenate it with the output generated by the same input token at the first time step of the backward net.\n",
        "\n",
        "**Note**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Keras also has a go_backwards keyword argument. If this is set to True, Keras automatically flips the input sequences and inputs them into the network in reverse order. This is the second half of a bidirectional layer.\n",
        "\n",
        "If you’re not using a bidirectional wrapper, this keyword can be useful, because a recurrent neural network (due to the vanishing gradients problem) is more receptive to data at the end of the sample than at the beginning. \n",
        "\n",
        "If you have padded your samples with <PAD> tokens at the end, all the good, juicy stuff is buried deep in the input loop. go_backwards can be a quick way around this problem.\n",
        "\n",
        "---\n",
        "\n",
        "Keras added a layer wrapper that will automatically flip\n",
        "around the necessary inputs and outputs to automatically assemble a bi-directional\n",
        "RNN for us.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RN0BzYyxl48",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "87c78a74-7827-43c6-b315-f888f5a9c626"
      },
      "source": [
        "from tensorflow.keras.layers import Bidirectional\n",
        "\n",
        "num_neurons = 10\n",
        "maxlen = 100\n",
        "embedding_dims = 300\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Bidirectional(SimpleRNN(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims))))\n",
        "model2.add(Dropout(0.2))\n",
        "\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model2.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "model2.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
        "\n",
        "model2.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/2\n",
            "20000/20000 [==============================] - 314s 16ms/sample - loss: 0.5419 - accuracy: 0.7267 - val_loss: 0.4660 - val_accuracy: 0.7864\n",
            "Epoch 2/2\n",
            "20000/20000 [==============================] - 308s 15ms/sample - loss: 0.4351 - accuracy: 0.8043 - val_loss: 0.4362 - val_accuracy: 0.8122\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_3 (Bidirection multiple                  6220      \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              multiple                  8001      \n",
            "=================================================================\n",
            "Total params: 14,221\n",
            "Trainable params: 14,221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96hOA7_gx-uj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_structure = model2.to_json()   # Note that this doesn’t save the weights of the network, only the structure.\n",
        "\n",
        "# Save your trained model before you lose it!\n",
        "with open('simple_rnn_model3.json', 'w') as json_file:\n",
        "  json_file.write(model_structure)\n",
        "model2.save_weights('simple_rnn_weights3.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj7CdvI2zKhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading model\n",
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "with open('simple_rnn_model3.json', 'r') as json_file:\n",
        "  json_string = json_file.read()\n",
        "model = model_from_json(json_string)\n",
        "\n",
        "model.load_weights('simple_rnn_weights3.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "448cB57qzQNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_1 = \"\"\"\n",
        "I'm hate that the dismal weather that had me down for so long, when will it break! Ugh, when does happiness return?  \n",
        "The sun is blinding and the puffy clouds are too thin.  I can't wait for the weekend.\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWmODzZhzQ4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
        "\n",
        "# Tokenize returns a list of the data (length 1 here)\n",
        "test_vec_list = pad_trunc(vec_list, maxlen)\n",
        "\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "model.predict(test_vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPcWL6wuzTgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.predict_classes(test_vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv0HfYeR1LvQ",
        "colab_type": "text"
      },
      "source": [
        "With these tools you’re well on your way to not just predicting and classifying text, but\n",
        "actually modeling language itself and how it’s used. And with that deeper algorithmic\n",
        "understanding, instead of just parroting text your model has seen before, you can\n",
        "generate completely new statements!"
      ]
    }
  ]
}